\section{Related Work}{\label{sec:related}}
Coresets are small summaries of data which 
can be used as a proxy to the original data with provable guarantees. The term was first introduced in \cite{agarwal2004approximating} where they used coresets for the shape fitting problem. Coresets for clustering problem were described in~\cite{har2004coresets}. 
In \cite{feldman2011unified} authors gave a generalized framework to construct coresets based on importance sampling using sensitivity scores introduced in \cite{langberg2010universal}. Interested reader can check \cite{woodruff2014sketching, braverman2016new, bachem2017practical}. Various online sampling schemes for spectral approximation are discussed in~\cite{cohen2016online, cohen2017input}. 

Tensor decomposition is unique under minimal assumptions \cite{kruskal1977three}. Therefore it has become very popular in various latent variable modeling applications \cite{anandkumar2012method, hsu2012spectral, anandkumar2014tensor}, neural networks \cite{janzamin2015beating} etc. However in general (i.e. without any assumption) most of the tensor problems including tensor decomposition are NP-hard \cite{hillar2013most}. There has been much work on fast tensor decomposition techniques. Various tensor sketching methods for tensor operations are discussed in \cite{bhojanapalli2015new, wang2015fast, song2016sublinear}. 
% They show that by applying FFT to the complete tensor during power iteration, one can save both time and space. This scheme can be used in combination with our scheme. 
The area of online tensor power iterations has also been explored in \cite{huang2015online, wang2016online}. 
% A work on element wise tensor sampling \cite{bhojanapalli2015new} gives a distribution on all the tensor elements and samples few entries accordingly. For $3^{rd}$ order, orthogonally decomposable tensors, \cite{song2016sublinear} gives a sub-linear time algorithm for tensor decomposition which requires the knowledge of norms of slices of the tensor. 
Various heuristics for tensor sketching as well as RandNLA techniques \cite{woodruff2014sketching} over matricized tensors for estimating low rank tensor approximation have been studied in \cite{song2019relative}.

In the online setting, for a matrix $\*A \in \~R^{n \times d}$ where rows are coming in streaming manner, the guarantee achieved by \cite{cohen2016online} while preserving additive error spectral approximation with sample size $O(d(\log d)(\log \epsilon\|\*A\|^{2}/\delta))$. $|\|\*A\*x\|^{2} - \|\*C\*x\|^{2}| \leq \epsilon \|\*A\*x\|^{2} + \delta, \forall \*x \in \~R^{d}$.

The problem of $\ell_{p}$ subspace embedding has been explored in both offline~\cite{dasgupta2009sampling, woodruff2013subspace, cohen2015p, clarkson2016fast} and streaming setting~\cite{dickens2018leveraging}. As any offline algorithm can be used as streaming algorithm~\cite{har2004coresets}, we use the known offline algorithms and summarize their results in streaming version in table \ref{tab:compare}.
% In \cite{dasgupta2009sampling} the authors show that one can spend $O(nd^{5}\log n)$ time to sample $O(\frac{d^{p+1}}{\epsilon^{2}})$ rows to get a guaranteed $(1\pm \epsilon)$ approximate subspace embedding for any $p$. 
The algorithm in~\cite{woodruff2013subspace} samples $O(n^{1-2/p}\mbox{poly}(d))$ rows and gives $\mbox{poly}(d)$ error relative subspace embedding but in $O(\mbox{nnz}(A))$ time. For streaming $\ell_{p}$ subspace embedding~\cite{dickens2018leveraging}, give a one pass deterministic algorithm for $\ell_{p}$ subspace embedding for $1\leq p\leq \infty$. 
%Note that our lower bound result shows that getting the number of row samples to be only a function of $d$ is not possible for $(1 \pm \epsilon)$ error in the one pass streaming / online setting. 
% As they also use well conditioned basis to decide sampling probability for the rows, hence it can also be used to preserve tensor contraction.
For some constant $\gamma \in (0,1)$ the algorithm takes $O(n^{\gamma}d)$ space and $O(n^{\gamma}d^{2}+n^{\gamma}d^{5}\log n)$ update time to return a $1/d^{O(1/\gamma)}$ error relative subspace embedding for any $\ell_{p}$ norm. 
% This, however, cannot be made into a constant factor approximation. We propose an online randomized algorithm that gives a guaranteed $(1\pm\epsilon)$ relative error approximation. 